<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Spark生态圈之Scala(二) ----- Scala中的数组操作</title>
      <link href="/2018/05/07/BigData-Scala1-2/"/>
      <url>/2018/05/07/BigData-Scala1-2/</url>
      <content type="html"><![CDATA[]]></content>
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Spark生态圈之Scala(一) ----- Scala中的函数</title>
      <link href="/2018/05/07/BigData-Scala1-1/"/>
      <url>/2018/05/07/BigData-Scala1-1/</url>
      <content type="html"><![CDATA[<h3 id="Scala中的函数"><a href="#Scala中的函数" class="headerlink" title="Scala中的函数"></a>Scala中的函数</h3><h4 id="Scala定义函数"><a href="#Scala定义函数" class="headerlink" title="Scala定义函数"></a>Scala定义函数</h4><p>在Scala中定义函数时，需要定义函数的函数名、参数、函数体<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def sayHello(name:String,age:Int):Int =</span><br><span class="line">&#123;</span><br><span class="line">  if(age&gt;18)</span><br><span class="line">  &#123;</span><br><span class="line">    printf(&quot;Hi,%s,you are a big boy!!!&quot;,name)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">    prinf(&quot;Hi,%s,you are a children&quot;,name)</span><br><span class="line">    age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>Scala要求必须给出所有参数的类型，但是不一定给出函数返回值的类型，只要右侧的函数体中不包含递归的语句，Scala就可以自己根据右侧的表达式推断出返回类型。</p><p>单行函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def sayHello(name:String):Unit = println(&quot;Hello&quot;+name)</span><br></pre></td></tr></table></figure></p><p>sum函数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def sum(n:Int):Int=</span><br><span class="line">&#123;</span><br><span class="line">  var result = 0</span><br><span class="line">  for(i&lt;- 1 to n)</span><br><span class="line">  &#123;</span><br><span class="line">    result +=i</span><br><span class="line">  &#125;</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>递归函数(斐波那契数列)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def fab(n:Int):Int =</span><br><span class="line">&#123;</span><br><span class="line">  if(n&lt;=0) 1</span><br><span class="line">  else fab(n-1)+fab(n-2)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="Scala函数中的默认参数"><a href="#Scala函数中的默认参数" class="headerlink" title="Scala函数中的默认参数"></a>Scala函数中的默认参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def sayHello(FirstName:String,MiddleName:String=&quot;&quot;,LastName:String=&quot;&quot;):Unit =</span><br><span class="line">print(FirstName+&quot; &quot;+MiddleName+&quot; &quot;+LastName)</span><br></pre></td></tr></table></figure><p>Java与Scala实现默认参数的区别:</p><p>Java:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public void sayHello(String name,int age)        </span><br><span class="line">&#123;                                                </span><br><span class="line">    if(name = null)                                </span><br><span class="line">   &#123;                                           </span><br><span class="line">      name = &quot;defaultName&quot;                        </span><br><span class="line">   &#125;</span><br><span class="line">   if(age ==0)</span><br><span class="line">    &#123;</span><br><span class="line">      age = 18</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Scala:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def sayHello(name:String,age:Int=20)</span><br><span class="line">&#123;</span><br><span class="line">    print(&quot;Hello,&quot;+name+&quot;, your age is &quot;+age)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="Scala函数中的带名参数"><a href="#Scala函数中的带名参数" class="headerlink" title="Scala函数中的带名参数"></a>Scala函数中的带名参数</h4><p>在调用函数时，也可以不按照函数定义的参数顺序来传递参数，而是使用带名参数的方式来传递<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sayHello(FirstName=&quot;Mick&quot;,LastName=&quot;Nina&quot;,MiddleName=&quot;jack&quot;)</span><br></pre></td></tr></table></figure></p><p>还可以混合使用未命名参数和带名参数，但是未命名参数必须排在带名参数前面<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sayHello(&quot;Mick&quot;,LastName=&quot;Nina&quot;,MiddleName=&quot;Jack&quot;)</span><br></pre></td></tr></table></figure></p><h4 id="Scala函数中的变长参数"><a href="#Scala函数中的变长参数" class="headerlink" title="Scala函数中的变长参数"></a>Scala函数中的变长参数</h4><p>在Scala中，有时我们需要将函数定义为参数个数可变的形式，则此时可以使用变长参数定义函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def sum(nums : Int*)</span><br><span class="line">&#123;</span><br><span class="line">    var res = 0</span><br><span class="line">    for(num &lt;- nums)</span><br><span class="line">       res += num</span><br><span class="line">    res</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用序列调用变长参数：</p><p>在如果想要将一个已有的序列直接调用变长参数函数，是不对的。比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val s = sum(1 to 5)</span><br></pre></td></tr></table></figure></p><p>此时需要使用Scala特殊的语法将参数定义为序列，让Scala解释器能够识别。这种语法非常有用！一定要好好注意，在Spark的源码中大量地使用到了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val s = sum(1 to 5:_*)</span><br></pre></td></tr></table></figure></p><p>案例：使用递归函数实现累加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def sum2(nums:Int*):Int=</span><br><span class="line">&#123;</span><br><span class="line">    if(nums.length == 0) 0</span><br><span class="line">     else nums.head+sum2(nums.tail : _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="Scala函数的过程、lazy值和异常"><a href="#Scala函数的过程、lazy值和异常" class="headerlink" title="Scala函数的过程、lazy值和异常"></a>Scala函数的过程、lazy值和异常</h4><p>1、过程:<br>  在Scala中，定义函数时，如果函数体直接包裹在了花括号里面，而没有使用=连接，则函数的返回值类型就是Unit。这样的函数就被称之为过程。过程通常用于不需要返回值的函数</p><p>  过程还有一种写法，就是将函数的返回值类型定义为Unit<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def sayHello(name:String) = &quot;Hello,&quot;+name</span><br><span class="line">def sayHello(name:String) &#123;print(&quot;Hello,&quot;+name);&quot;Hello,&quot;+name&#125;</span><br><span class="line">def sayHello(name:String):Unit = &quot;Hello,&quot;+name</span><br></pre></td></tr></table></figure></p><p>2、lazy值：<br>  在Scala中，提供了lazy值的特性，也就是说，如果将一个变量声明为lazy，则只有在第一次使用该变量时，变量对应的表达式才会发生计算。这种特性对于特别耗时的计算操作特别有用，比如打开文件进行IO，进行网络IO等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import scala.io.Source._</span><br><span class="line">lazy val lines = fromFile(&quot;C://Users//Administrator//Desktop//test.txt&quot;).mkString</span><br></pre></td></tr></table></figure><p>即使文件不存在，也不会报错，只有第一个使用变量时会报错，证明了表达式计算的lazy特性<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val lines = fromFile(&quot;C://Users//Administrator//Desktop//test.txt&quot;).mkString</span><br><span class="line">lazy val lines = fromFile(&quot;C://Users//Administractor//Desktop//test.txt&quot;).mkString</span><br><span class="line">def lines = fromFile(&quot;C://Users//Administrator//Desktop//test.txt&quot;).mkString</span><br></pre></td></tr></table></figure></p><p>3、异常:<br>  在Scala中，异常处理和捕获机制与Java是非常相似的</p>]]></content>
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>随笔(一)</title>
      <link href="/2018/05/02/jottings-%E4%B8%80/"/>
      <url>/2018/05/02/jottings-%E4%B8%80/</url>
      <content type="html"><![CDATA[<p><img src="/2018/05/02/jottings-一/jottings-1.jpg" alt="jottings"><br><a id="more"></a><br><strong>或许世界不是你预期的模样<br>柴米油盐包裹着大胆的梦想<br>学会了成熟，一直保持善良<br>勇敢哭、用力笑、别逞强<br>不勉强追寻飞到多高的地方<br>只要你快乐才是唯一的真相<br>当你扮大人，扮到疲惫了<br>有个家让你做回孩子。</strong></p>]]></content>
      
      <categories>
          
          <category> thoughts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jottings </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop生态圈(二) ----- Kafka的使用(四)</title>
      <link href="/2018/04/28/BigData-Kafka-%E5%9B%9B/"/>
      <url>/2018/04/28/BigData-Kafka-%E5%9B%9B/</url>
      <content type="html"><![CDATA[<p>  <img src="/2018/04/28/BigData-Kafka-四/kafka6.png" alt="Kafka拦截器"><br><a id="more"></a></p><h3 id="Kafka-producer拦截器-interceptor"><a href="#Kafka-producer拦截器-interceptor" class="headerlink" title="Kafka producer拦截器(interceptor)"></a>Kafka producer拦截器(interceptor)</h3><h4 id="拦截器原理"><a href="#拦截器原理" class="headerlink" title="拦截器原理"></a>拦截器原理</h4><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。</p><p>　　对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：</p><p>  （1）onSend(ProducerRecord)：<br>  该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算</p><p>  （2）onAcknowledgement(RecordMetadata, Exception)：<br>  该方法会在消息被应答之前或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率</p><p>  （3）close：<br>  关闭interceptor，主要用于执行一些资源清理工作</p><p>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p><h4 id="拦截器案例"><a href="#拦截器案例" class="headerlink" title="拦截器案例"></a>拦截器案例</h4><p>  1）需求：<br>  实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。<br>  <img src="/2018/04/28/BigData-Kafka-四/kafka6.png" alt="Kafka拦截器"></p><p>  2）案例实操<br>    (1)增加时间戳拦截器<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    package com.atguigu.kafka.interceptor;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;</span><br><span class="line">// 创建一个新的record，把时间戳写入消息体的最前部</span><br><span class="line">return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),</span><br><span class="line">System.currentTimeMillis() + &quot;,&quot; + record.value().toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.interceptor;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123;</span><br><span class="line">    private int errorCounter = 0;</span><br><span class="line">    private int successCounter = 0;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;</span><br><span class="line"> return record;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">// 统计成功和失败的次数</span><br><span class="line">        if (exception == null) &#123;</span><br><span class="line">            successCounter++;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            errorCounter++;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() &#123;</span><br><span class="line">        // 保存结果</span><br><span class="line">        System.out.println(&quot;Successful sent: &quot; + successCounter);</span><br><span class="line">        System.out.println(&quot;Failed sent: &quot; + errorCounter);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>（3）producer主程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.interceptor;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line">public class InterceptorProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">// 1 设置配置信息</span><br><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">props.put(&quot;retries&quot;, 0);</span><br><span class="line">props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">// 2 构建拦截链</span><br><span class="line">List&lt;String&gt; interceptors = new ArrayList&lt;&gt;();</span><br><span class="line">interceptors.add(&quot;com.atguigu.kafka.interceptor.TimeInterceptor&quot;); interceptors.add(&quot;com.atguigu.kafka.interceptor.CounterInterceptor&quot;);</span><br><span class="line">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br><span class="line"></span><br><span class="line">String topic = &quot;first&quot;;</span><br><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">// 3 发送消息</span><br><span class="line">for (int i = 0; i &lt; 10; i++) &#123;</span><br><span class="line"></span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i);</span><br><span class="line">    producer.send(record).get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 4 一定要关闭producer，这样才会调用interceptor的close方法</span><br><span class="line">producer.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>4）测试<br>（1）在kafka上启动消费者，然后运行客户端java程序。</p><blockquote><p>[atguigu@hadoop102 kafka]$ in/kafka-console-consumer.sh –zookeeper hadoop102:2181 –from-beginning –topic first<br>1501904047034,message0<br>1501904047225,message1<br>1501904047230,message2<br>1501904047234,message3<br>1501904047236,message4<br>1501904047240,message5<br>1501904047243,message6<br>1501904047246,message7<br>1501904047249,message8<br>1501904047252,message9</p></blockquote><p>（2）观察java平台控制台输出数据如下：</p><blockquote><p>Successful sent: 10<br>Failed sent: 0</p></blockquote><h3 id="Kafka-stream"><a href="#Kafka-stream" class="headerlink" title="Kafka stream"></a>Kafka stream</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>1) Kafka Stream<br>  Kafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。它建立在流处理的一系列重要功能基础之上，比如正确区分事件和处理时间，处理迟到数据以及高效的应用程序状态管理。</p><p>2)  Kafka Stream 特点<br>  ① 功能强大<br>  高拓展性，弹性，容错<br>  有状态和无状态处理<br>  基于事件时间的Window，Join，Aggergations<br>  ② 轻量级<br>  无需专门的集群<br>  一个库，而不是框架<br>  ③ 完全集成<br>  100%的Kafka 0.10.0版本兼容<br>  易于集成到现有的应用程序<br>  程序部署无需手工处理(这个指的应该是Kafka多分区机制对Kafka Streams多实例的自动匹配)<br>  ④ 实时性<br>  毫秒级延迟<br>  并非微批处理<br>  窗口允许乱序数据<br>  允许迟到数据</p><p>3) 为什么要有Kafka Stream<br>当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如MapR，Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。<br>既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？笔者认为主要有如下原因。<br>  第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。<br>  第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。更为重要的是，Kafka Stream充分利用了Kafka的分区机制和Consumer的Rebalance机制，使得Kafka Stream可以非常方便的水平扩展，并且各个实例可以使用不同的部署方式。具体来说，每个运行Kafka Stream的应用程序实例都包含了Kafka Consumer实例，多个同一应用的实例之间并行处理数据集。而不同实例之间的部署方式并不要求一致，比如部分实例可以运行在Web容器中，部分实例可运行在Docker或Kubernetes中。<br>  第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。<br>  第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。<br>  第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。<br>  第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。</p><h4 id="Kafka-Stream案例"><a href="#Kafka-Stream案例" class="headerlink" title="Kafka Stream案例"></a>Kafka Stream案例</h4><p>  去除单词前缀“&gt;&gt;&gt;”案例<br>  0）需求：<br>    实时处理单词带有”&gt;&gt;&gt;”前缀的内容。例如输入”atguigu&gt;&gt;&gt;jingjing”，最终处理成“jingjing”</p><p>  1）创建主类<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">  package com.atguigu.kafka.stream;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.streams.KafkaStreams;</span><br><span class="line">import org.apache.kafka.streams.StreamsConfig;</span><br><span class="line">import org.apache.kafka.streams.processor.Processor;</span><br><span class="line">import org.apache.kafka.streams.processor.ProcessorSupplier;</span><br><span class="line">import org.apache.kafka.streams.processor.TopologyBuilder;</span><br><span class="line"></span><br><span class="line">public class Application &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">// 定义输入的topic</span><br><span class="line">        String from = &quot;first&quot;;</span><br><span class="line">        // 定义输出的topic</span><br><span class="line">        String to = &quot;second&quot;;</span><br><span class="line"></span><br><span class="line">        // 设置参数</span><br><span class="line">        Properties settings = new Properties();</span><br><span class="line">        settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;logFilter&quot;);</span><br><span class="line">        settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;hadoop102:9092&quot;);</span><br><span class="line"></span><br><span class="line">        StreamsConfig config = new StreamsConfig(settings);</span><br><span class="line"></span><br><span class="line">        // 构建拓扑</span><br><span class="line">        TopologyBuilder builder = new TopologyBuilder();</span><br><span class="line"></span><br><span class="line">        builder.addSource(&quot;SOURCE&quot;, from)</span><br><span class="line">               .addProcessor(&quot;PROCESS&quot;, new ProcessorSupplier&lt;byte[], byte[]&gt;() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Processor&lt;byte[], byte[]&gt; get() &#123;</span><br><span class="line">// 具体分析处理</span><br><span class="line">return new LogProcessor();</span><br><span class="line">&#125;</span><br><span class="line">&#125;, &quot;SOURCE&quot;)</span><br><span class="line">                .addSink(&quot;SINK&quot;, to, &quot;PROCESS&quot;);</span><br><span class="line"></span><br><span class="line">        // 创建kafka stream</span><br><span class="line">        KafkaStreams streams = new KafkaStreams(builder, config);</span><br><span class="line">        streams.start();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>  2)具体业务处理<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">  package com.atguigu.kafka.stream;</span><br><span class="line">import org.apache.kafka.streams.processor.Processor;</span><br><span class="line">import org.apache.kafka.streams.processor.ProcessorContext;</span><br><span class="line"></span><br><span class="line">public class LogProcessor implements Processor&lt;byte[], byte[]&gt; &#123;</span><br><span class="line"></span><br><span class="line">private ProcessorContext context;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void init(ProcessorContext context) &#123;</span><br><span class="line">this.context = context;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void process(byte[] key, byte[] value) &#123;</span><br><span class="line">String input = new String(value);</span><br><span class="line"></span><br><span class="line">// 如果包含“&gt;&gt;&gt;”则只保留该标记后面的内容</span><br><span class="line">if (input.contains(&quot;&gt;&gt;&gt;&quot;)) &#123;</span><br><span class="line">input = input.split(&quot;&gt;&gt;&gt;&quot;)[1].trim();</span><br><span class="line">// 输出到下一个topic</span><br><span class="line">context.forward(&quot;logProcessor&quot;.getBytes(), input.getBytes());</span><br><span class="line">&#125;else&#123;</span><br><span class="line">context.forward(&quot;logProcessor&quot;.getBytes(), input.getBytes());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void punctuate(long timestamp) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>3）将程序用eclipse插件打成jar包</p><p>4）将jar包拷贝hadoop102上运行</p><blockquote><p>[atguigu@hadoop102 kafka]$ java -jar kafka0508_fat.jar com.atguigu.kafka.stream.Application</p></blockquote><p>5）在hadoop104上启动生产者</p><blockquote><p>[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh –broker-list hadoop102:9092 –topic first<br>$&gt;hello&gt;&gt;&gt;world<br>$&gt;h&gt;&gt;&gt;atguigu<br>$&gt;hahaha</p></blockquote><p>6）在hadoop103山启动消费者</p><blockquote><p>[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh –zookeeper hadoop102:2181 –from-beginning –topic second<br>world<br>atguigu<br>hahaha</p></blockquote>]]></content>
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop生态圈(二) ----- Kafka的使用(三)</title>
      <link href="/2018/04/28/BigData-Kafka-%E4%B8%89/"/>
      <url>/2018/04/28/BigData-Kafka-%E4%B8%89/</url>
      <content type="html"><![CDATA[<h3 id="Kafka-API操作"><a href="#Kafka-API操作" class="headerlink" title="Kafka API操作"></a>Kafka API操作</h3><h4 id="创建生产者"><a href="#创建生产者" class="headerlink" title="创建生产者"></a>创建生产者</h4><p>过时的API:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import kafka.javaapi.producer.Producer;</span><br><span class="line">import kafka.producer.KeyedMessage;</span><br><span class="line">import kafka.producer.ProducerConfig;</span><br><span class="line"></span><br><span class="line">public class OldProducer &#123;</span><br><span class="line"></span><br><span class="line">@SuppressWarnings(&quot;deprecation&quot;)</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">Properties properties = new Properties();</span><br><span class="line">properties.put(&quot;metadata.broker.list&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">properties.put(&quot;request.required.acks&quot;, &quot;1&quot;);</span><br><span class="line">properties.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);</span><br><span class="line"></span><br><span class="line">Producer&lt;Integer, String&gt; producer = new Producer&lt;Integer,String&gt;(new ProducerConfig(properties));</span><br><span class="line"></span><br><span class="line">KeyedMessage&lt;Integer, String&gt; message = new KeyedMessage&lt;Integer, String&gt;(&quot;first&quot;, &quot;hello world&quot;);</span><br><span class="line">producer.send(message );</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>新的API:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line">public class NewProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">Properties props = new Properties();</span><br><span class="line">// Kafka服务端的主机名和端口号</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;hadoop103:9092&quot;);</span><br><span class="line">// 等待所有副本节点的应答</span><br><span class="line">props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">// 消息发送最大尝试次数</span><br><span class="line">props.put(&quot;retries&quot;, 0);</span><br><span class="line">// 一批消息处理大小</span><br><span class="line">props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">// 请求延时</span><br><span class="line">props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">// 发送缓存区内存大小</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">// key序列化</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">// value序列化</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">for (int i = 0; i &lt; 50; i++) &#123;</span><br><span class="line">producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), &quot;hello world-&quot; + i));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">producer.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="创建生产者带回调函数"><a href="#创建生产者带回调函数" class="headerlink" title="创建生产者带回调函数"></a>创建生产者带回调函数</h4><p>新API:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.producer.Callback;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class CallBackProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">Properties props = new Properties();</span><br><span class="line">// Kafka服务端的主机名和端口号</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;hadoop103:9092&quot;);</span><br><span class="line">// 等待所有副本节点的应答</span><br><span class="line">props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">// 消息发送最大尝试次数</span><br><span class="line">props.put(&quot;retries&quot;, 0);</span><br><span class="line">// 一批消息处理大小</span><br><span class="line">props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">// 增加服务端请求延时</span><br><span class="line">props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">// 发送缓存区内存大小</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">// key序列化</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">// value序列化</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 50; i++) &#123;</span><br><span class="line"></span><br><span class="line">kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, &quot;hello&quot; + i), new Callback() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line"></span><br><span class="line">if (metadata != null) &#123;</span><br><span class="line"></span><br><span class="line">System.err.println(metadata.partition() + &quot;---&quot; + metadata.offset());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kafkaProducer.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="自定义分区生产者"><a href="#自定义分区生产者" class="headerlink" title="自定义分区生产者"></a>自定义分区生产者</h4><p>  0) 需求:将所有数据存储到topic的第0将所有数据存储到topic的第0号分区上</p><p>  1) 定义一个类实现Partitioner接口，重写里面的方法(过时API)<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">  package com.atguigu.kafka;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import kafka.producer.Partitioner;</span><br><span class="line"></span><br><span class="line">public class CustomPartitioner implements Partitioner &#123;</span><br><span class="line"></span><br><span class="line">public CustomPartitioner() &#123;</span><br><span class="line">super();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int partition(Object key, int numPartitions) &#123;</span><br><span class="line">// 控制分区</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>  2) 自定义分区(新API)<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">  package com.atguigu.kafka;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.Partitioner;</span><br><span class="line">import org.apache.kafka.common.Cluster;</span><br><span class="line"></span><br><span class="line">public class CustomPartitioner implements Partitioner &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        // 控制分区</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>  3) 在代码中调用<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">  package com.atguigu.kafka;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line">public class PartitionerProducer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">Properties props = new Properties();</span><br><span class="line">// Kafka服务端的主机名和端口号</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;hadoop103:9092&quot;);</span><br><span class="line">// 等待所有副本节点的应答</span><br><span class="line">props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">// 消息发送最大尝试次数</span><br><span class="line">props.put(&quot;retries&quot;, 0);</span><br><span class="line">// 一批消息处理大小</span><br><span class="line">props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">// 增加服务端请求延时</span><br><span class="line">props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">// 发送缓存区内存大小</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">// key序列化</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">// value序列化</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">// 自定义分区</span><br><span class="line">props.put(&quot;partitioner.class&quot;, &quot;com.atguigu.kafka.CustomPartitioner&quot;);</span><br><span class="line"></span><br><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, &quot;1&quot;, &quot;atguigu&quot;));</span><br><span class="line"></span><br><span class="line">producer.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>  4) 测试<br>  （1）在hadoop102上监控/opt/module/kafka/logs/目录下first主题3个分区的log日志动态变化情况</p><blockquote><p>[atguigu@hadoop102 first-0]$ tail -f 00000000000000000000.log<br>  [atguigu@hadoop102 first-1]$ tail -f 00000000000000000000.log<br>  [atguigu@hadoop102 first-2]$ tail -f 00000000000000000000.log</p></blockquote><p>  （2）发现数据都存储到指定的分区了。</p><h4 id="Kafka-消费者API"><a href="#Kafka-消费者API" class="headerlink" title="Kafka 消费者API"></a>Kafka 消费者API</h4><p>0）在控制台创建发送者</p><blockquote><p>[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh –broker-list hadoop102:9092 –topic first<br>$&gt;hello world</p></blockquote><p>1）创建消费者（过时API）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.consume;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import kafka.consumer.Consumer;</span><br><span class="line">import kafka.consumer.ConsumerConfig;</span><br><span class="line">import kafka.consumer.ConsumerIterator;</span><br><span class="line">import kafka.consumer.KafkaStream;</span><br><span class="line">import kafka.javaapi.consumer.ConsumerConnector;</span><br><span class="line"></span><br><span class="line">public class CustomConsumer &#123;</span><br><span class="line"></span><br><span class="line">@SuppressWarnings(&quot;deprecation&quot;)</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">Properties properties = new Properties();</span><br><span class="line"></span><br><span class="line">properties.put(&quot;zookeeper.connect&quot;, &quot;hadoop102:2181&quot;);</span><br><span class="line">properties.put(&quot;group.id&quot;, &quot;g1&quot;);</span><br><span class="line">properties.put(&quot;zookeeper.session.timeout.ms&quot;, &quot;500&quot;);</span><br><span class="line">properties.put(&quot;zookeeper.sync.time.ms&quot;, &quot;250&quot;);</span><br><span class="line">properties.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line"></span><br><span class="line">// 创建消费者连接器</span><br><span class="line">ConsumerConnector consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(properties));</span><br><span class="line"></span><br><span class="line">HashMap&lt;String, Integer&gt; topicCount = new HashMap&lt;&gt;();</span><br><span class="line">topicCount.put(&quot;first&quot;, 1);</span><br><span class="line"></span><br><span class="line">Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCount);</span><br><span class="line"></span><br><span class="line">KafkaStream&lt;byte[], byte[]&gt; stream = consumerMap.get(&quot;first&quot;).get(0);</span><br><span class="line"></span><br><span class="line">ConsumerIterator&lt;byte[], byte[]&gt; it = stream.iterator();</span><br><span class="line"></span><br><span class="line">while (it.hasNext()) &#123;</span><br><span class="line">System.out.println(new String(it.next().message()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>2）官方提供案例（自动维护消费情况）（新API）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.consume;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line">import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line">public class CustomNewConsumer &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">Properties props = new Properties();</span><br><span class="line">// 定义kakfa 服务的地址，不需要将所有broker指定上</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">// 制定consumer group</span><br><span class="line">props.put(&quot;group.id&quot;, &quot;test&quot;);</span><br><span class="line">// 是否自动确认offset</span><br><span class="line">props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);</span><br><span class="line">// 自动确认offset的时间间隔</span><br><span class="line">props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line">// key的序列化类</span><br><span class="line">props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">// value的序列化类</span><br><span class="line">props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">// 定义consumer</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">// 消费者订阅的topic, 可同时订阅多个</span><br><span class="line">consumer.subscribe(Arrays.asList(&quot;first&quot;, &quot;second&quot;,&quot;third&quot;));</span><br><span class="line"></span><br><span class="line">while (true) &#123;</span><br><span class="line">// 读取数据，读取超时时间为100ms</span><br><span class="line">ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);</span><br><span class="line"></span><br><span class="line">for (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop生态圈(二) ----- Kafka的使用(二)</title>
      <link href="/2018/04/28/BigData-Kafka-%E4%BA%8C/"/>
      <url>/2018/04/28/BigData-Kafka-%E4%BA%8C/</url>
      <content type="html"><![CDATA[<p><img src="/2018/04/28/BigData-Kafka-二/kafka1.png" alt="Kafka内部实现原理"><br><a id="more"></a></p><h3 id="Kafka工作流程分析"><a href="#Kafka工作流程分析" class="headerlink" title="Kafka工作流程分析"></a>Kafka工作流程分析</h3><hr><h4 id="Kafka生产过程分析"><a href="#Kafka生产过程分析" class="headerlink" title="Kafka生产过程分析"></a>Kafka生产过程分析</h4><p>  1) 写入方式<br>  producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。</p><p>  2) 分区<br>  消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：<br>  <img src="/2018/04/28/BigData-Kafka-二/kafka2.png" alt="分区"></p><p>  <img src="/2018/04/28/BigData-Kafka-二/kafka3.png" alt="分区"><br>  我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。</p><p>  ① 分区的原因<br>  （1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；<br>  （2）可以提高并发，因为可以以Partition为单位读写了。</p><p>  ② 分区的原则<br>  （1）指定了patition，则直接使用；<br>  （2）未指定patition但指定key，通过对key的value进行hash出一个patition</p><p>  ③ patition和key都未指定，使用轮询选出一个patition。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">DefaultPartitioner类</span><br><span class="line">public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster)</span><br><span class="line">&#123;</span><br><span class="line">      List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">      int numPartitions = partitions.size();</span><br><span class="line">      if (keyBytes == null) &#123;</span><br><span class="line">          int nextValue = nextValue(topic);</span><br><span class="line">          List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">          if (availablePartitions.size() &gt; 0) &#123;</span><br><span class="line">              int part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">              return availablePartitions.get(part).partition();</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">              // no partitions are available, give a non-available partition</span><br><span class="line">              return Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">          // hash the keyBytes to choose a partition</span><br><span class="line">          return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>  3) 副本<br>  同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。</p><p>  4) 写入流程<br>   producer写入消息流程如下：</p><p>  <img src="/2018/04/28/BigData-Kafka-二/kafka4.png" alt="写入流程"></p><p>  ① producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader<br>  ② producer将消息发送给该leader<br>  ③ leader将消息写入本地log<br>  ④ followers从leader pull消息，写入本地log后向leader发送ACK<br>  ⑤ leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</p><h4 id="broker保存消息"><a href="#broker保存消息" class="headerlink" title="broker保存消息"></a>broker保存消息</h4><p>  1)存储方式<br>  物理上把topic分成一个或多个partition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件)</p><p>  2)存储策略<br>  无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：<br>  1）基于时间：log.retention.hours=168<br>  2）基于大小：log.retention.bytes=1073741824<br>  需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</p><p>  3)Zookeeper存储结构<br><img src="/2018/04/28/BigData-Kafka-二/zookeeper.png" alt="写入流程"><br>注意：producer不在zk中注册，消费者在zk中注册。</p><h4 id="Kafka消费过程分析"><a href="#Kafka消费过程分析" class="headerlink" title="Kafka消费过程分析"></a>Kafka消费过程分析</h4><p>kafka提供了两套consumer API：高级Consumer API和低级API。<br>1) 高级API<br>  (1)高级API优点<br>  高级API 写起来简单不需要去自行去管理offset，系统通过zookeeper自行管理不需要管理分区，副本等情况，系统自动管理消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的的offset）可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）<br>  (2)高级API缺点<br>  不能自行控制offset（对于某些特殊需求来说）<br>  不能细化控制如分区、副本、zk等</p><p>2) 低级API<br>  (1)低级API优点<br>  能够开发者自己控制offset，想从哪里读取就从哪里读取。自行控制连接分区，对分区自定义进行负载均衡对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）<br>  (2)低级API缺点<br>  太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。</p><p>3) 消费者组<br><img src="/2018/04/28/BigData-Kafka-二/kafka5.png" alt="消费者组"><br>  消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的grouop，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。</p><p>4) 消费方式<br><strong>consumer采用pull（拉）模式从broker中读取数据。</strong><br>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。<br>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p><p>5) 消费者组案例<br>(1)需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。<br>(2)案例实操<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">① 在hadoop102、hadoop103上修改/opt/module/kafka/config/consumer.properties配置文件中的group.id属性为任意组名。</span><br><span class="line">[atguigu@hadoop103 config]$ vi consumer.properties group.id=atguigu</span><br><span class="line">② 在hadoop102、hadoop103上分别启动消费者</span><br><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties</span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties</span><br><span class="line">③ 在hadoop104上启动生产者</span><br><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first</span><br><span class="line">hello world</span><br><span class="line">④ 查看hadoop102和hadoop103的接收者。同一时刻只有一个消费者接收到消息。</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop生态圈(一) ----- Flume的使用</title>
      <link href="/2018/04/23/BigData-Flume/"/>
      <url>/2018/04/23/BigData-Flume/</url>
      <content type="html"><![CDATA[<p><img src="/2018/04/23/BigData-Flume/agent1.jpg" alt="单个Agent采集数据"><br><a id="more"></a></p><h3 id="Flume在集群中扮演的角色"><a href="#Flume在集群中扮演的角色" class="headerlink" title="Flume在集群中扮演的角色"></a>Flume在集群中扮演的角色</h3><h4 id="Flume框架简介"><a href="#Flume框架简介" class="headerlink" title="Flume框架简介"></a>Flume框架简介</h4><p>  ① Flume提供一个分布式的，可靠的，对大数据量的日志进行高效收集、聚集、移动的服务，Flume只能在Unix环境下运行。<br>  ②Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、HBase、hive、kafka等众多外部存储系统中<br>  ③一般的采集需求，通过对flume的简单配置即可实现<br>  ④Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景</p><h4 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h4><p>  1、Flume分布式系统中最核心的角色是agent，Flume采集系统就是由一个个agent所连接起来形成<br>  2、每一个agent相当于一个数据传递员，内部有三个组件:<br>    a) Source:采集源，用于跟数据源对接，以获取数据<br>    b) Sink:下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据<br>    c) Channel:angent内部的数据传输通道，用于从source将数据传递到sink<br>    <strong>Source 到 Channel 到 Sink之间传递数据的形式是Event事件；Event事件是一个数据流单元。</strong></p><ul><li><p>简单结构<br><strong>单个Agent采集数据</strong><br><img src="/2018/04/23/BigData-Flume/agent1.jpg" alt="单个Agent采集数据"></p></li><li><p>复杂结构<br><strong>多个Agent采集数据</strong><br><img src="/2018/04/23/BigData-Flume/agent2.png" alt="多个Agent采集数据"></p></li></ul><h4 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h4><ul><li>Source<br>  用于采集数据，Source是生产数据流的地方，同时Source会将产生的数据流传输到Channel，这个有点类似于Java IO部分的Channel</li><li>Channel<br>  用于桥接Sources和Sinks，类似于一个队列</li><li>Sink<br>  从Channel收集数据，将数据写到目标源(可以是下一个Source，也可以是HDFS或者HBase)</li></ul><h4 id="传输单元"><a href="#传输单元" class="headerlink" title="传输单元"></a>传输单元</h4><ul><li>Event<br>  Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地</li></ul><h4 id="传输过程"><a href="#传输过程" class="headerlink" title="传输过程"></a>传输过程</h4><p>source监控某个文件，文件产生新的数据，拿到该数据后，将数据封装在一个Event中，并put<br>到channel后commit提交，channel队列先出先进，Sink去Channel队列中拉取数据，然后写入<br>到HDFS或者HBase中。</p><h3 id="安装配置Flume"><a href="#安装配置Flume" class="headerlink" title="安装配置Flume"></a>安装配置Flume</h3><p>  flume-env.sh<br>  配置java的环境变量<br>  进入Flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME</p><h3 id="Flume帮助命令"><a href="#Flume帮助命令" class="headerlink" title="Flume帮助命令"></a>Flume帮助命令</h3><p>  $bin/flume-ng</p><h3 id="案例讲解"><a href="#案例讲解" class="headerlink" title="案例讲解"></a>案例讲解</h3><h4 id="案例一-Flume监听端口，输出端口数据"><a href="#案例一-Flume监听端口，输出端口数据" class="headerlink" title="案例一:Flume监听端口，输出端口数据"></a>案例一:Flume监听端口，输出端口数据</h4><p>  ① 创建Flume Agent配置文件flume-telnet.修改conf<br>  ② cp -a fllume-conf.properties.template flume-telnet.conf<br>  ③ 进入文件，写入如下内容<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> # Name the components on this agent（起名）</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source（监听端口的方式）</span><br><span class="line">a1.sources.r1.type = netcat（源数据的数据类型）</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></p><p>  a1是指agent缩写<br>  ④ 安装telnet工具(没有反应正常)<br>    $ sudo rpm -ivh telnet-server-0.17-59.el7.x86_64.rpm<br>    $ sudo rpm -ivh telnet-0.17-59.el7.x86_64.rpm</p><p>  ⑤首先判断44444端口是否被占用<br>    $ netstat -an | grep 44444</p><p>  ⑥先开启flume先听端口(flume家目录执行)<br>    $ bin/flume-ng agent –conf conf/ –name a1 –conf-file conf/flume-telnet.conf -Dflume.root.logger==INFO,console</p><p>  ⑦使用telnet工具向本机的44444端口发送内容(新开一个窗口执行，不需非要家目录，本窗口输入信息。另外窗口可以监听到）<br>    $ telnet localhost 44444</p><h4 id="案例二-监听上传Hive日志文件到HDFS"><a href="#案例二-监听上传Hive日志文件到HDFS" class="headerlink" title="案例二:监听上传Hive日志文件到HDFS"></a>案例二:监听上传Hive日志文件到HDFS</h4><p>  ① 拷贝Hadoop相关jar到Flume的lib目录下<br>    share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar<br>    share/hadoop/common/lib/commons-configuration-1.6.jar<br>    share/hadoop/mapreduce1/lib/hadoop-hdfs-2.5.0-cdh5.3.6.jar<br>    share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6.jar<br>    $cp share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar  /opt/modules/apache-flume-1.5.0-cdh5.3.6-bin/lib<br>  ② 创建flume-hdfs.conf文件<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">  # Name the components on this agent</span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line">a2.sources.r2.command = tail -f /opt/modules/cdh/hive-0.13.1-cdh5.3.6/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://192.168.122.20:8020/flume/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = events-hive-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a2.sinks.k2.hdfs.minBlockReplicas = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure></p><h4 id="案例三-Flume监听整个目录"><a href="#案例三-Flume监听整个目录" class="headerlink" title="案例三:Flume监听整个目录"></a>案例三:Flume监听整个目录</h4><p>① 创建配置文件flume-dir.conf<br>$ cp -a flume-hdfs.conf flume-dir.conf<br>② 文件内容<br>      <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">  a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/modules/cdh/apache-flume-1.5.0-cdh5.3.6-bin/upload</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line">#忽略所有以.tmp结尾的文件，不上传</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://192.168.122.20:8020/flume/upload/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a3.sinks.k3.hdfs.minBlockReplicas = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></p><p>③ 执行测试<br>$ bin/flume-ng agent –conf conf/ –name a3 –conf-file conf/flume-dir.conf &amp;</p>]]></content>
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hadoop生态圈(二) ----- Kafka的使用(一)</title>
      <link href="/2018/04/23/BigData-Kafka/"/>
      <url>/2018/04/23/BigData-Kafka/</url>
      <content type="html"><![CDATA[<p><img src="/2018/04/23/BigData-Kafka/kafka.png" alt="Kafka内部实现原理"><br><a id="more"></a></p><h3 id="Kafka概述"><a href="#Kafka概述" class="headerlink" title="Kafka概述"></a>Kafka概述</h3><hr><h4 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h4><p>  在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。<br>  1）Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。<br>  2）Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。<br>  3）Kafka是一个分布式消息队列:生产者，消费者的功能。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。<br>  4）无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。</p><h4 id="Kafka内部实现原理"><a href="#Kafka内部实现原理" class="headerlink" title="Kafka内部实现原理"></a>Kafka内部实现原理</h4><p>  <img src="/2018/04/23/BigData-Kafka/kafka.png" alt="Kafka内部实现原理"><br>  （1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）<br>  点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到,l客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。<br>  （2）发布/订阅模式（一对多，数据生产后，推送给所有订阅者）<br>  发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。</p><h4 id="为什么需要消息队列"><a href="#为什么需要消息队列" class="headerlink" title="为什么需要消息队列"></a>为什么需要消息队列</h4><p>  1）解耦：<br>  　　允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。<br>  2）冗余：<br>  消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。<br>  3）扩展性：<br>  因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。<br>  4）灵活性 &amp; 峰值处理能力：<br>  在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。<br>  5）可恢复性：<br>  系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。<br>  6）顺序保证：<br>  在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）<br>  7）缓冲：<br>  有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。<br>  8）异步通信：<br>  很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p><h4 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h4><p>  <img src="/2018/04/23/BigData-Kafka/kafka1.png" alt="Kafka架构"></p><p>  1）Producer ：消息生产者，就是向kafka broker发消息的客户端。<br>  2）Consumer ：消息消费者，向kafka broker取消息的客户端<br>  3）Topic ：可以理解为一个队列。<br>  4） Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。<br>  5）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。<br>  6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。<br>  7）Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</p><h4 id="kafka命令行操作"><a href="#kafka命令行操作" class="headerlink" title="kafka命令行操作"></a>kafka命令行操作</h4><p>  1) 查看当前服务器中的所有topic</p><blockquote><p><strong>$</strong> bin/kafka-topics.sh --list --zookeeper hadoop102:2181</p></blockquote><p>  2) 创建topic</p><blockquote><p><strong>$</strong> bin/kafka-topics.sh --create --zookeeper hadoop102:2181 –replication-factor 3 –partitions 1 –topic first<br>    选项说明:<br>    --topic 定义topic名<br>    --replication-factor 定义副本数<br>    --partitions 定义分区数</p></blockquote><p>  3)  删除topic</p><blockquote><p><strong>$</strong> bin/kafka-topics.sh --delete --zookeeper hadoop102:2181 --topic first<br>  需要server.properties中设置delete.topic.enable=true否则只会标记删除或者直接重启</p></blockquote><p>  4) 发送消息</p><blockquote><p><strong>$</strong> bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first<br>    $&gt;hello world<br>    $&gt;kongluo kongluo</p></blockquote><p>  5) 消费消息</p><blockquote><p><strong>$</strong> bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --from-beginning --topic first</p></blockquote><p>  6) 查看某个Topic的详情</p><blockquote><p><strong>$</strong> bin/kafka-topics.sh --topic first --describe --zookeeper hadoop102:2181</p></blockquote>]]></content>
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>myFirstPageForTag</title>
      <link href="/2018/04/21/myFirstPageForTag/"/>
      <url>/2018/04/21/myFirstPageForTag/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      
        <tags>
            
            <tag> Scala </tag>
            
            <tag> java </tag>
            
            <tag> HDFS </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>我的第一篇博客</title>
      <link href="/2018/04/20/first-one/"/>
      <url>/2018/04/20/first-one/</url>
      <content type="html"><![CDATA[<p><img src="/2018/04/20/first-one/bye.jpg" alt="Bye"><br><a id="more"></a></p><p><i class="icon-pencil"></i> <strong>个人感想</strong></p><h4 id="第一点"><a href="#第一点" class="headerlink" title="  第一点"></a><i class="icon-file"></i>  <strong>第一点</strong></h4><p>我很庆幸在2016毕业的那一年去了北京，我更庆幸在2018年离开了北京。走出校园后，经过这两年的社会的洗礼，我已然能轻易的分清楚所有的善与恶，好与坏，真与假。</p><h4 id="第二点"><a href="#第二点" class="headerlink" title="  第二点"></a><i class="icon-file"></i>  <strong>第二点</strong></h4><p>这辈子很短，<strong>真诚</strong>，比什么都重要。</p><h4 id="第三点"><a href="#第三点" class="headerlink" title="  第三点"></a><i class="icon-file"></i>  <strong>第三点</strong></h4><p><strong>实力</strong> 才是硬道理，永远不要做语言的巨人。</p><h4 id="第四点"><a href="#第四点" class="headerlink" title="  第四点"></a><i class="icon-file"></i>  <strong>第四点</strong></h4><p>不要给那些让你觉得恶心的人任何有机可乘的机会，因为时间很宝贵，要把时间给那些带给你温暖的人。</p>]]></content>
      
      <categories>
          
          <category> thoughts </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>article title</title>
      <link href="/2018/04/20/article-title/"/>
      <url>/2018/04/20/article-title/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
  
  
</search>
