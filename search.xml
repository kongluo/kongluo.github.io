<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Hadoop生态圈(一) ----- Flume的使用</title>
      <link href="/2018/04/23/BigData-Flume/"/>
      <url>/2018/04/23/BigData-Flume/</url>
      <content type="html"><![CDATA[<h3 id="Flume在集群中扮演的角色"><a href="#Flume在集群中扮演的角色" class="headerlink" title="Flume在集群中扮演的角色"></a>Flume在集群中扮演的角色</h3><h4 id="Flume框架简介"><a href="#Flume框架简介" class="headerlink" title="Flume框架简介"></a>Flume框架简介</h4><p>  ① Flume提供一个分布式的，可靠的，对大数据量的日志进行高效收集、聚集、移动的服务，Flume<br>  只能在Unix环境下运行。<br>  ②Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、<br>  HBase、hive、kafka等众多外部存储系统中<br>  ③一般的采集需求，通过对flume的简单配置即可实现<br>  ④Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景</p><h4 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h4><p>  1、Flume分布式系统中最核心的角色是agent，Flume采集系统就是由一个个agent所连接起来形成<br>  2、每一个agent相当于一个数据传递员，内部有三个组件:<br>    a) Source:采集源，用于跟数据源对接，以获取数据<br>    b) Sink:下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据<br>    c) Channel:angent内部的数据传输通道，用于从source将数据传递到sink<br>    <strong>Source 到 Channel 到 Sink之间传递数据的形式是Event事件；Event事件是一个数据流单元。</strong></p><ul><li><p>简单结构<br><strong>单个Agent采集数据</strong><br><img src="/2018/04/23/BigData-Flume/agent1.jpg" alt="单个Agent采集数据"></p></li><li><p>复杂结构<br><strong>多个Agent采集数据</strong><br><img src="/2018/04/23/BigData-Flume/agent2.png" alt="多个Agent采集数据"></p></li></ul><h4 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h4><ul><li>Source<br>  用于采集数据，Source是生产数据流的地方，同时Source会将产生的数据流传输到Channel，这个有点类似于Java IO部分的Channel</li><li>Channel<br>  用于桥接Sources和Sinks，类似于一个队列</li><li>Sink<br>  从Channel收集数据，将数据写到目标源(可以是下一个Source，也可以是HDFS或者HBase)</li></ul><h4 id="传输单元"><a href="#传输单元" class="headerlink" title="传输单元"></a>传输单元</h4><ul><li>Event<br>  Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地</li></ul><h4 id="传输过程"><a href="#传输过程" class="headerlink" title="传输过程"></a>传输过程</h4><p>source监控某个文件，文件产生新的数据，拿到该数据后，将数据封装在一个Event中，并put<br>到channel后commit提交，channel队列先出先进，Sink去Channel队列中拉取数据，然后写入<br>到HDFS或者HBase中。</p><h3 id="安装配置Flume"><a href="#安装配置Flume" class="headerlink" title="安装配置Flume"></a>安装配置Flume</h3><p>  flume-env.sh<br>  配置java的环境变量<br>  进入Flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME</p><h3 id="Flume帮助命令"><a href="#Flume帮助命令" class="headerlink" title="Flume帮助命令"></a>Flume帮助命令</h3><p>  $bin/flume-ng</p><h3 id="案例讲解"><a href="#案例讲解" class="headerlink" title="案例讲解"></a>案例讲解</h3><h4 id="案例一-Flume监听端口，输出端口数据"><a href="#案例一-Flume监听端口，输出端口数据" class="headerlink" title="案例一:Flume监听端口，输出端口数据"></a>案例一:Flume监听端口，输出端口数据</h4><p>  ① 创建Flume Agent配置文件flume-telnet.修改conf<br>  ② cp -a fllume-conf.properties.template flume-telnet.conf<br>  ③ 进入文件，写入如下内容<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> # Name the components on this agent（起名）</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source（监听端口的方式）</span><br><span class="line">a1.sources.r1.type = netcat（源数据的数据类型）</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></p><p>  a1是指agent缩写<br>  ④ 安装telnet工具(没有反应正常)<br>    $ sudo rpm -ivh telnet-server-0.17-59.el7.x86_64.rpm<br>    $ sudo rpm -ivh telnet-0.17-59.el7.x86_64.rpm</p><p>  ⑤首先判断44444端口是否被占用<br>    $ netstat -an | grep 44444</p><p>  ⑥先开启flume先听端口(flume家目录执行)<br>    $ bin/flume-ng agent –conf conf/ –name a1 –conf-file conf/flume-telnet.conf -Dflume.root.logger==INFO,console</p><p>  ⑦使用telnet工具向本机的44444端口发送内容(新开一个窗口执行，不需非要家目录，本窗口输入信息。另外窗口可以监听到）<br>    $ telnet localhost 44444</p><h4 id="案例二-监听上传Hive日志文件到HDFS"><a href="#案例二-监听上传Hive日志文件到HDFS" class="headerlink" title="案例二:监听上传Hive日志文件到HDFS"></a>案例二:监听上传Hive日志文件到HDFS</h4><p>  ① 拷贝Hadoop相关jar到Flume的lib目录下<br>    share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar<br>    share/hadoop/common/lib/commons-configuration-1.6.jar<br>    share/hadoop/mapreduce1/lib/hadoop-hdfs-2.5.0-cdh5.3.6.jar<br>    share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6.jar<br>    $cp share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar  /opt/modules/apache-flume-1.5.0-cdh5.3.6-bin/lib<br>  ② 创建flume-hdfs.conf文件<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">  # Name the components on this agent</span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line">a2.sources.r2.command = tail -f /opt/modules/cdh/hive-0.13.1-cdh5.3.6/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://192.168.122.20:8020/flume/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = events-hive-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a2.sinks.k2.hdfs.minBlockReplicas = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure></p><h4 id="案例三-Flume监听整个目录"><a href="#案例三-Flume监听整个目录" class="headerlink" title="案例三:Flume监听整个目录"></a>案例三:Flume监听整个目录</h4><p>① 创建配置文件flume-dir.conf<br>$ cp -a flume-hdfs.conf flume-dir.conf<br>② 文件内容<br>      <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">  a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/modules/cdh/apache-flume-1.5.0-cdh5.3.6-bin/upload</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line">#忽略所有以.tmp结尾的文件，不上传</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://192.168.122.20:8020/flume/upload/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a3.sinks.k3.hdfs.minBlockReplicas = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></p><p>③ 执行测试<br>$ bin/flume-ng agent –conf conf/ –name a3 –conf-file conf/flume-dir.conf &amp;</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Hadoop生态圈(二) ----- Kafka的使用</title>
      <link href="/2018/04/23/BigData-Kafka/"/>
      <url>/2018/04/23/BigData-Kafka/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
    <entry>
      <title>myFirstPageForTag</title>
      <link href="/2018/04/21/myFirstPageForTag/"/>
      <url>/2018/04/21/myFirstPageForTag/</url>
      <content type="html"><![CDATA[]]></content>
      
      <categories>
          
          <category> thoughts </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> Scala </tag>
            
            <tag> HDFS </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>我的第一篇博客</title>
      <link href="/2018/04/20/first-one/"/>
      <url>/2018/04/20/first-one/</url>
      <content type="html"><![CDATA[<p><i class="icon-pencil"></i> <strong>个人感想</strong></p><h4 id="第一点"><a href="#第一点" class="headerlink" title="  第一点"></a><i class="icon-file"></i>  <strong>第一点</strong></h4><p>我很庆幸在2016毕业的那一年去了北京，我更庆幸在2018年离开了北京。走出校园后，经过这两年的社会的洗礼，我已然能轻易的分清楚所有的善与恶，好与坏，真与假。<br><a id="more"></a></p><h4 id="第二点"><a href="#第二点" class="headerlink" title="  第二点"></a><i class="icon-file"></i>  <strong>第二点</strong></h4><p>这辈子很短，<strong>真诚</strong>，比什么都重要。</p><h4 id="第三点"><a href="#第三点" class="headerlink" title="  第三点"></a><i class="icon-file"></i>  <strong>第三点</strong></h4><p><strong>实力</strong> 才是硬道理，永远不要做语言的巨人。</p><h4 id="第四点"><a href="#第四点" class="headerlink" title="  第四点"></a><i class="icon-file"></i>  <strong>第四点</strong></h4><p>不要给那些让你觉得恶心的人任何有机可乘的机会，因为时间很宝贵，要把时间给那些带给你温暖的人。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>article title</title>
      <link href="/2018/04/20/article-title/"/>
      <url>/2018/04/20/article-title/</url>
      <content type="html"><![CDATA[]]></content>
      
      
    </entry>
    
  
  
</search>
